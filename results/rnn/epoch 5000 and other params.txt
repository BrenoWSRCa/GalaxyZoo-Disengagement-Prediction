epoch 5000 and other params
/Users/brenocarvalho/miniconda3/envs/xai_phd_test/bin/python /Users/brenocarvalho/PycharmProjects/xai_phd_test/rnn_predict.py
Preparing dataset: 100%|██████████| 1085/1085 [00:00<00:00, 1372.37it/s]
Preparing dataset: 100%|██████████| 357/357 [00:00<00:00, 1868.30it/s]
Epoch:   0%|          | 1/5000 [00:06<9:13:33,  6.64s/it]Epoch     0: loss(0.5626305938), auc(0.5115019252)
Epoch:   2%|▏         | 101/5000 [07:42<6:21:05,  4.67s/it]Epoch   100: loss(0.2907439768), auc(0.5000000000)
Epoch:   4%|▍         | 204/5000 [14:57<5:35:45,  4.20s/it]Epoch   200: loss(0.2476125807), auc(0.5408083633)
Epoch:   6%|▌         | 301/5000 [22:56<6:18:54,  4.84s/it]Epoch   300: loss(0.2375248224), auc(0.5947150961)
Epoch:   8%|▊         | 401/5000 [30:50<6:16:04,  4.91s/it]Epoch   400: loss(0.2292286307), auc(0.6104903817)
Epoch:  10%|█         | 501/5000 [39:20<6:16:45,  5.02s/it]Epoch   500: loss(0.2226600647), auc(0.6128692025)
Epoch:  12%|█▏        | 601/5000 [47:19<5:30:28,  4.51s/it]Epoch   600: loss(0.2180459499), auc(0.6147211570)
Epoch:  14%|█▍        | 701/5000 [55:22<6:07:23,  5.13s/it]Epoch   700: loss(0.2151559740), auc(0.6168533807)
Epoch:  16%|█▌        | 801/5000 [1:03:28<5:43:55,  4.91s/it]Epoch   800: loss(0.2135064155), auc(0.6229182842)
Epoch:  18%|█▊        | 901/5000 [1:11:12<4:51:42,  4.27s/it]Epoch   900: loss(0.2125507444), auc(0.6270549763)
Epoch:  20%|██        | 1007/5000 [1:18:38<4:38:31,  4.19s/it]Epoch  1000: loss(0.2118444443), auc(0.6310302413)
Epoch:  22%|██▏       | 1108/5000 [1:25:35<4:27:56,  4.13s/it]Epoch  1100: loss(0.2111375332), auc(0.6337992600)
Epoch:  24%|██▍       | 1205/5000 [1:32:15<4:22:04,  4.14s/it]Epoch  1200: loss(0.2104325294), auc(0.6337230031)
Epoch:  26%|██▌       | 1307/5000 [1:39:17<4:12:46,  4.11s/it]Epoch  1300: loss(0.2098246813), auc(0.6346063953)
Epoch:  28%|██▊       | 1410/5000 [1:46:28<4:06:58,  4.13s/it]Epoch  1400: loss(0.2092759162), auc(0.6342072842)
Epoch:  30%|███       | 1507/5000 [1:53:09<4:00:44,  4.14s/it]Epoch  1500: loss(0.2086901218), auc(0.6328059389)
Epoch:  32%|███▏      | 1611/5000 [2:00:21<3:53:20,  4.13s/it]Epoch  1600: loss(0.2080658823), auc(0.6344875533)
Epoch:  34%|███▍      | 1708/5000 [2:07:05<3:47:27,  4.15s/it]Epoch  1700: loss(0.2076640874), auc(0.6381825492)
Epoch:  36%|███▌      | 1808/5000 [2:14:00<3:42:26,  4.18s/it]Epoch  1800: loss(0.2081505805), auc(0.6404336149)
Epoch:  38%|███▊      | 1906/5000 [2:20:46<3:31:49,  4.11s/it]Epoch  1900: loss(0.2101064175), auc(0.6429223645)
Epoch:  40%|████      | 2007/5000 [2:27:45<3:27:01,  4.15s/it]Epoch  2000: loss(0.2127774209), auc(0.6448931610)
Epoch:  42%|████▏     | 2107/5000 [2:34:39<3:19:33,  4.14s/it]Epoch  2100: loss(0.2144799381), auc(0.6495141343)
Epoch:  44%|████▍     | 2208/5000 [2:41:40<3:17:41,  4.25s/it]Epoch  2200: loss(0.2147195935), auc(0.6526911772)
Epoch:  46%|████▌     | 2305/5000 [2:48:19<3:06:29,  4.15s/it]Epoch  2300: loss(0.2143732905), auc(0.6520543821)
Epoch:  48%|████▊     | 2407/5000 [2:55:20<2:58:11,  4.12s/it]Epoch  2400: loss(0.2136013061), auc(0.6498884866)
Epoch:  50%|█████     | 2508/5000 [3:02:20<2:51:16,  4.12s/it]Epoch  2500: loss(0.2123097777), auc(0.6464737597)
Epoch:  52%|█████▏    | 2610/5000 [3:09:21<2:44:13,  4.12s/it]Epoch  2600: loss(0.2109379917), auc(0.6455903675)
Epoch:  54%|█████▍    | 2707/5000 [3:16:02<2:39:53,  4.18s/it]Epoch  2700: loss(0.2099965364), auc(0.6446643902)
Epoch:  56%|█████▌    | 2809/5000 [3:23:03<2:30:19,  4.12s/it]Epoch  2800: loss(0.2098603249), auc(0.6440612670)
Epoch:  58%|█████▊    | 2906/5000 [3:29:44<2:24:40,  4.15s/it]Epoch  2900: loss(0.2104633600), auc(0.6443415361)
Epoch:  60%|██████    | 3007/5000 [3:36:41<2:16:51,  4.12s/it]Epoch  3000: loss(0.2113536149), auc(0.6416487743)
Epoch:  62%|██████▏   | 3109/5000 [3:43:47<2:10:47,  4.15s/it]Epoch  3100: loss(0.2125248760), auc(0.6428124356)
Epoch:  64%|██████▍   | 3207/5000 [3:50:32<2:03:28,  4.13s/it]Epoch  3200: loss(0.2150847465), auc(0.6400434169)
Epoch:  66%|██████▌   | 3311/5000 [3:57:44<1:57:01,  4.16s/it]Epoch  3300: loss(0.2180197984), auc(0.6395591358)
Epoch:  68%|██████▊   | 3409/5000 [4:04:29<1:49:01,  4.11s/it]Epoch  3400: loss(0.2203809619), auc(0.6370278011)
Epoch:  70%|███████   | 3506/5000 [4:11:11<1:43:39,  4.16s/it]Epoch  3500: loss(0.2225786448), auc(0.6377071812)
Epoch:  72%|███████▏  | 3610/5000 [4:18:22<1:36:55,  4.18s/it]Epoch  3600: loss(0.2238856107), auc(0.6379874503)
Epoch:  74%|███████▍  | 3711/5000 [4:25:21<1:28:43,  4.13s/it]Epoch  3700: loss(0.2247013003), auc(0.6382677193)
Epoch:  76%|███████▌  | 3808/5000 [4:32:02<1:22:02,  4.13s/it]Epoch  3800: loss(0.2249588221), auc(0.6408753110)
Epoch:  78%|███████▊  | 3910/5000 [4:39:05<1:15:29,  4.16s/it]Epoch  3900: loss(0.2249398381), auc(0.6420815573)
Epoch:  80%|████████  | 4007/5000 [4:45:46<1:07:54,  4.10s/it]Epoch  4000: loss(0.2240927964), auc(0.6400345038)
Epoch:  82%|████████▏ | 4109/5000 [4:52:46<1:01:01,  4.11s/it]Epoch  4100: loss(0.2230884582), auc(0.6391936966)
Epoch:  84%|████████▍ | 4206/5000 [4:59:26<54:57,  4.15s/it]Epoch  4200: loss(0.2218952328), auc(0.6389134276)
Epoch:  86%|████████▌ | 4307/5000 [5:06:25<47:27,  4.11s/it]Epoch  4300: loss(0.2203251570), auc(0.6410793231)
Epoch:  88%|████████▊ | 4409/5000 [5:13:25<41:40,  4.23s/it]Epoch  4400: loss(0.2182866335), auc(0.6415210192)
Epoch:  90%|█████████ | 4506/5000 [5:20:07<34:09,  4.15s/it]Epoch  4500: loss(0.2166848630), auc(0.6404761999)
Epoch:  92%|█████████▏| 4607/5000 [5:27:05<27:03,  4.13s/it]Epoch  4600: loss(0.2154788226), auc(0.6398730767)
Epoch:  94%|█████████▍| 4709/5000 [5:34:05<20:06,  4.15s/it]Epoch  4700: loss(0.2144798487), auc(0.6395502226)
Epoch:  96%|█████████▌| 4806/5000 [5:40:44<13:22,  4.13s/it]Epoch  4800: loss(0.2137197703), auc(0.6383439763)
Epoch:  98%|█████████▊| 4913/5000 [5:48:12<05:59,  4.13s/it]Epoch  4900: loss(0.2132195681), auc(0.6380211222)
Epoch: 100%|██████████| 5000/5000 [5:54:13<00:00,  4.25s/it]
RNN:
              precision    recall  f1-score   support

         0.0       0.56      0.30      0.39    4902.0
         1.0       0.47      0.73      0.57    4222.0

    accuracy                           0.50    9124.0
   macro avg       0.52      0.51      0.48    9124.0
weighted avg       0.52      0.50      0.47    9124.0

RNN - best validation AUC:
              precision    recall  f1-score   support

         0.0       0.62      0.35      0.45    4902.0
         1.0       0.50      0.76      0.60    4222.0

    accuracy                           0.54    9124.0
   macro avg       0.56      0.55      0.52    9124.0
weighted avg       0.57      0.54      0.52    9124.0

RNN - best loss:
              precision    recall  f1-score   support

         0.0       0.53      0.80      0.63    4902.0
         1.0       0.42      0.17      0.24    4222.0

    accuracy                           0.51    9124.0
   macro avg       0.47      0.48      0.44    9124.0
weighted avg       0.48      0.51      0.45    9124.0

                                       accuracy       AUC
name                      window_size                    
RNN                       --           0.496786   0.52751
RNN - best validation AUC --           0.524386  0.552245
RNN - best loss           --           0.535728  0.486693
All zeroes                --           0.535728       0.5
All ones                  --           0.535728       0.5

Process finished with exit code 0
